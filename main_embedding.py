"""
AI Text Embedding Generator - Main Entry Point

A comprehensive text embedding system supporting Zhipu GLM embedding models.
Provides both MCP server capabilities and direct text embedding functionality.
"""

import os
import sys
import time
import json
import uuid
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any, Union
from mcp.server.fastmcp import FastMCP
from zhipu_embedding_client import ZhipuEmbeddingClient
from network_diagnostic import NetworkDiagnostic

# Create an MCP server
mcp = FastMCP("AI Text Embedding Generator")

# Create directories for storing files
OUTPUTS_DIR = Path("outputs")
OUTPUTS_DIR.mkdir(exist_ok=True)

# Initialize clients
embedding_client = ZhipuEmbeddingClient(api_key=os.getenv("ZHIPU_API_KEY"))

# Text Embedding Entry Point
class EmbeddingGenerator:
    """ä¸»è¦çš„æ–‡æœ¬åµŒå…¥å…¥å£ç±»"""
    
    def __init__(self):
        self.embedding_client = embedding_client
        self.outputs_dir = OUTPUTS_DIR
    
    def get_embeddings(self, 
                      input_text: Union[str, List[str]],
                      model: str = "embedding-3") -> Dict[str, Any]:
        """
        ä¸»è¦çš„æ–‡æœ¬åµŒå…¥å…¥å£
        
        Args:
            input_text: è¾“å…¥æ–‡æœ¬ï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
            model: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ (embedding-3, embedding-2)
            
        Returns:
            åµŒå…¥ç»“æœ
        """
        return self.embedding_client.get_embeddings(
            input_text=input_text,
            model=model
        )
    
    def get_single_embedding(self, text: str, model: str = "embedding-3") -> List[float]:
        """è·å–å•ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        return self.embedding_client.get_single_embedding(text, model)
    
    def get_batch_embeddings(self, texts: List[str], model: str = "embedding-3") -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬åµŒå…¥å‘é‡"""
        return self.embedding_client.get_batch_embeddings(texts, model)
    
    def calculate_similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        return self.embedding_client.calculate_similarity(embedding1, embedding2)
    
    def find_most_similar(self, query_text: str, candidate_texts: List[str], 
                         model: str = "embedding-3") -> List[Dict[str, Any]]:
        """æ‰¾åˆ°ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬"""
        return self.embedding_client.find_most_similar(query_text, candidate_texts, model)

# åˆ›å»ºå…¨å±€æ–‡æœ¬åµŒå…¥å®ä¾‹
embedding_generator = EmbeddingGenerator()

@mcp.tool()
def get_text_embeddings(
    input_text: Union[str, List[str]],
    model: str = "embedding-3"
) -> Dict[str, Any]:
    """
    è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡è¡¨ç¤º
    
    Args:
        input_text: è¾“å…¥æ–‡æœ¬ï¼Œå¯ä»¥æ˜¯å•ä¸ªå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
        model: åµŒå…¥æ¨¡å‹åç§° (embedding-3, embedding-2)
    
    Returns:
        åŒ…å«åµŒå…¥å‘é‡çš„ç»“æœå­—å…¸
    """
    try:
        if not input_text:
            return {
                "success": False,
                "error": "è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
            }
        
        result = embedding_generator.get_embeddings(
            input_text=input_text,
            model=model
        )
        
        return {
            "success": True,
            "model": result.get("model"),
            "data": result.get("data"),
            "usage": result.get("usage")
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ–‡æœ¬åµŒå…¥å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def get_batch_embeddings(
    texts: List[str],
    model: str = "embedding-3"
) -> Dict[str, Any]:
    """
    æ‰¹é‡è·å–å¤šä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        model: åµŒå…¥æ¨¡å‹åç§° (embedding-3, embedding-2)
    
    Returns:
        åŒ…å«æ‰¹é‡åµŒå…¥å‘é‡çš„ç»“æœå­—å…¸
    """
    try:
        if not texts:
            return {
                "success": False,
                "error": "æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
            }
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text.strip() for text in texts if text and text.strip()]
        
        if not valid_texts:
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬"
            }
        
        embeddings = embedding_generator.get_batch_embeddings(
            texts=valid_texts,
            model=model
        )
        
        return {
            "success": True,
            "model": model,
            "embeddings": embeddings,
            "count": len(embeddings),
            "dimension": len(embeddings[0]) if embeddings else 0
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ‰¹é‡æ–‡æœ¬åµŒå…¥å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def calculate_text_similarity(
    text1: str,
    text2: str,
    model: str = "embedding-3"
) -> Dict[str, Any]:
    """
    è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦
    
    Args:
        text1: ç¬¬ä¸€ä¸ªæ–‡æœ¬
        text2: ç¬¬äºŒä¸ªæ–‡æœ¬
        model: åµŒå…¥æ¨¡å‹åç§° (embedding-3, embedding-2)
    
    Returns:
        åŒ…å«ç›¸ä¼¼åº¦åˆ†æ•°çš„ç»“æœå­—å…¸
    """
    try:
        if not text1 or not text2:
            return {
                "success": False,
                "error": "ä¸¤ä¸ªæ–‡æœ¬éƒ½ä¸èƒ½ä¸ºç©º"
            }
        
        # è·å–ä¸¤ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡
        embedding1 = embedding_generator.get_single_embedding(text1, model)
        embedding2 = embedding_generator.get_single_embedding(text2, model)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = embedding_generator.calculate_similarity(embedding1, embedding2)
        
        return {
            "success": True,
            "text1": text1,
            "text2": text2,
            "similarity": similarity,
            "model": model,
            "interpretation": {
                "score": similarity,
                "level": "é«˜" if similarity > 0.8 else "ä¸­" if similarity > 0.5 else "ä½",
                "description": f"ä¸¤ä¸ªæ–‡æœ¬çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸º {similarity:.4f}"
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def find_similar_texts(
    query_text: str,
    candidate_texts: List[str],
    model: str = "embedding-3",
    top_k: int = 5
) -> Dict[str, Any]:
    """
    åœ¨å€™é€‰æ–‡æœ¬ä¸­æ‰¾åˆ°ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„æ–‡æœ¬
    
    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬
        candidate_texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
        model: åµŒå…¥æ¨¡å‹åç§° (embedding-3, embedding-2)
        top_k: è¿”å›æœ€ç›¸ä¼¼çš„å‰kä¸ªç»“æœ
    
    Returns:
        åŒ…å«ç›¸ä¼¼æ–‡æœ¬æ’åºç»“æœçš„å­—å…¸
    """
    try:
        if not query_text:
            return {
                "success": False,
                "error": "æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º"
            }
        
        if not candidate_texts:
            return {
                "success": False,
                "error": "å€™é€‰æ–‡æœ¬åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
            }
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_candidates = [text.strip() for text in candidate_texts if text and text.strip()]
        
        if not valid_candidates:
            return {
                "success": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å€™é€‰æ–‡æœ¬"
            }
        
        # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æœ¬
        results = embedding_generator.find_most_similar(
            query_text=query_text,
            candidate_texts=valid_candidates,
            model=model
        )
        
        # é™åˆ¶è¿”å›ç»“æœæ•°é‡
        top_results = results[:min(top_k, len(results))]
        
        return {
            "success": True,
            "query": query_text,
            "model": model,
            "total_candidates": len(valid_candidates),
            "top_k": len(top_results),
            "results": top_results
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ç›¸ä¼¼æ–‡æœ¬æœç´¢å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def get_supported_embedding_models() -> Dict[str, Any]:
    """
    è·å–æ”¯æŒçš„åµŒå…¥æ¨¡å‹åˆ—è¡¨
    
    Returns:
        åŒ…å«æ”¯æŒæ¨¡å‹çš„ç»“æœå­—å…¸
    """
    try:
        models = embedding_client.get_available_models()
        
        return {
            "success": True,
            "models": models,
            "default_model": "embedding-3",
            "model_info": {
                "embedding-3": "æœ€æ–°çš„åµŒå…¥æ¨¡å‹ï¼Œæä¾›é«˜è´¨é‡çš„æ–‡æœ¬å‘é‡è¡¨ç¤º",
                "embedding-2": "è¾ƒæ—©ç‰ˆæœ¬çš„åµŒå…¥æ¨¡å‹ï¼Œå…¼å®¹æ€§æ›´å¥½"
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–æ”¯æŒçš„æ¨¡å‹å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def test_embedding_api(test_text: Optional[str] = None) -> Dict[str, Any]:
    """
    æµ‹è¯•æ–‡æœ¬åµŒå…¥APIè¿æ¥å’ŒåŠŸèƒ½
    
    Args:
        test_text: å¯é€‰çš„æµ‹è¯•æ–‡æœ¬
    
    Returns:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    try:
        # æµ‹è¯•APIè¿æ¥
        connection_test = embedding_client.test_connection()
        
        result = {
            "success": True,
            "connection_test": connection_test,
            "supported_models": embedding_client.get_available_models()
        }
        
        # å¦‚æœæä¾›äº†æµ‹è¯•æ–‡æœ¬ï¼Œè¿›è¡ŒåµŒå…¥æµ‹è¯•
        if test_text:
            try:
                embedding = embedding_client.get_single_embedding(test_text, "embedding-3")
                result["embedding_test"] = {
                    "success": True,
                    "text": test_text,
                    "dimension": len(embedding),
                    "sample_values": embedding[:5] if len(embedding) >= 5 else embedding
                }
            except Exception as e:
                result["embedding_test"] = {
                    "success": False,
                    "error": str(e)
                }
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "error": f"APIæµ‹è¯•å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def save_embeddings_to_file(
    texts: List[str],
    filename: str,
    model: str = "embedding-3"
) -> Dict[str, Any]:
    """
    å°†æ–‡æœ¬åµŒå…¥å‘é‡ä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        texts: æ–‡æœ¬åˆ—è¡¨
        filename: ä¿å­˜çš„æ–‡ä»¶å
        model: åµŒå…¥æ¨¡å‹åç§°
    
    Returns:
        ä¿å­˜ç»“æœå­—å…¸
    """
    try:
        if not texts or not filename:
            return {
                "success": False,
                "error": "æ–‡æœ¬åˆ—è¡¨å’Œæ–‡ä»¶åéƒ½æ˜¯å¿…éœ€çš„"
            }
        
        # è·å–åµŒå…¥å‘é‡
        embeddings = embedding_generator.get_batch_embeddings(texts, model)
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            "model": model,
            "timestamp": time.time(),
            "count": len(texts),
            "dimension": len(embeddings[0]) if embeddings else 0,
            "data": [
                {
                    "text": text,
                    "embedding": embedding
                }
                for text, embedding in zip(texts, embeddings)
            ]
        }
        
        # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())[:8]
        name, ext = os.path.splitext(filename)
        if not ext:
            ext = ".json"
        unique_filename = f"{name}_{file_id}{ext}"
        file_path = OUTPUTS_DIR / unique_filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "file_path": str(file_path),
            "filename": unique_filename,
            "size": file_path.stat().st_size,
            "count": len(texts),
            "dimension": save_data["dimension"],
            "model": model
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ä¿å­˜åµŒå…¥å‘é‡å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def load_embeddings_from_file(filename: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶åŠ è½½æ–‡æœ¬åµŒå…¥å‘é‡
    
    Args:
        filename: æ–‡ä»¶å
    
    Returns:
        åŠ è½½ç»“æœå­—å…¸
    """
    try:
        if not filename:
            return {
                "success": False,
                "error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"
            }
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = Path(filename)
        if not file_path.exists():
            # å°è¯•åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾
            output_path = OUTPUTS_DIR / file_path.name
            if output_path.exists():
                file_path = output_path
            else:
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}"
                }
        
        # åŠ è½½æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return {
            "success": True,
            "filename": file_path.name,
            "model": data.get("model"),
            "count": data.get("count"),
            "dimension": data.get("dimension"),
            "timestamp": data.get("timestamp"),
            "data": data.get("data", [])
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"åŠ è½½åµŒå…¥å‘é‡å¤±è´¥: {str(e)}"
        }

def run_interactive_mode():
    """è¿è¡Œäº¤äº’å¼æ–‡æœ¬åµŒå…¥æ¨¡å¼"""
    print("=" * 60)
    print("ğŸ”¤ AIæ–‡æœ¬åµŒå…¥ç”Ÿæˆå™¨ - äº¤äº’æ¨¡å¼")
    print("=" * 60)
    print("æ”¯æŒçš„åŠŸèƒ½:")
    print("1. æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆ")
    print("2. æ‰¹é‡æ–‡æœ¬åµŒå…¥")
    print("3. æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—")
    print("4. ç›¸ä¼¼æ–‡æœ¬æœç´¢")
    print("5. æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹")
    print("6. æµ‹è¯•APIè¿æ¥")
    print("7. ä¿å­˜åµŒå…¥å‘é‡åˆ°æ–‡ä»¶")
    print("8. ä»æ–‡ä»¶åŠ è½½åµŒå…¥å‘é‡")
    print("9. å¯åŠ¨MCPæœåŠ¡å™¨")
    print("0. é€€å‡º")
    print("=" * 60)
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (0-9): ").strip()
            
            if choice == "0":
                print("ğŸ‘‹ å†è§!")
                break
            elif choice == "1":
                handle_text_embedding()
            elif choice == "2":
                handle_batch_embedding()
            elif choice == "3":
                handle_similarity_calculation()
            elif choice == "4":
                handle_similar_text_search()
            elif choice == "5":
                handle_model_info()
            elif choice == "6":
                handle_api_test()
            elif choice == "7":
                handle_save_embeddings()
            elif choice == "8":
                handle_load_embeddings()
            elif choice == "9":
                print("ğŸ”§ å¯åŠ¨MCPæœåŠ¡å™¨...")
                mcp.run(transport="sse")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥0-9")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

def handle_text_embedding():
    """å¤„ç†æ–‡æœ¬åµŒå…¥"""
    print("\nğŸ”¤ æ–‡æœ¬åµŒå…¥å‘é‡ç”Ÿæˆ")
    text = input("è¯·è¾“å…¥è¦ç”ŸæˆåµŒå…¥å‘é‡çš„æ–‡æœ¬: ").strip()
    if not text:
        print("âŒ æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        return
    
    models = embedding_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: embedding-3): ").strip() or "embedding-3"
    
    print("ğŸ” ç”Ÿæˆä¸­...")
    try:
        result = get_text_embeddings(text, model)
        
        if result["success"]:
            data = result["data"][0] if result["data"] else {}
            embedding = data.get("embedding", [])
            
            print(f"âœ… åµŒå…¥å‘é‡ç”ŸæˆæˆåŠŸ!")
            print(f"æ–‡æœ¬: {text}")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"å‘é‡ç»´åº¦: {len(embedding)}")
            print(f"å‰5ä¸ªå€¼: {embedding[:5]}")
            print(f"ä½¿ç”¨æƒ…å†µ: {result.get('usage', {})}")
        else:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")

def handle_batch_embedding():
    """å¤„ç†æ‰¹é‡æ–‡æœ¬åµŒå…¥"""
    print("\nğŸ“ æ‰¹é‡æ–‡æœ¬åµŒå…¥")
    print("è¯·è¾“å…¥è¦ç”ŸæˆåµŒå…¥å‘é‡çš„æ–‡æœ¬ (æ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    
    texts = []
    while True:
        text = input().strip()
        if not text:
            break
        texts.append(text)
    
    if not texts:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•æ–‡æœ¬")
        return
    
    models = embedding_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: embedding-3): ").strip() or "embedding-3"
    
    print(f"ğŸ” æ‰¹é‡ç”Ÿæˆ {len(texts)} ä¸ªæ–‡æœ¬çš„åµŒå…¥å‘é‡...")
    try:
        result = get_batch_embeddings(texts, model)
        
        if result["success"]:
            print(f"âœ… æ‰¹é‡åµŒå…¥ç”ŸæˆæˆåŠŸ!")
            print(f"æ–‡æœ¬æ•°é‡: {result['count']}")
            print(f"å‘é‡ç»´åº¦: {result['dimension']}")
            print(f"æ¨¡å‹: {result['model']}")
        else:
            print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç”Ÿæˆå¤±è´¥: {str(e)}")

def handle_similarity_calculation():
    """å¤„ç†ç›¸ä¼¼åº¦è®¡ç®—"""
    print("\nğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—")
    text1 = input("è¯·è¾“å…¥ç¬¬ä¸€ä¸ªæ–‡æœ¬: ").strip()
    text2 = input("è¯·è¾“å…¥ç¬¬äºŒä¸ªæ–‡æœ¬: ").strip()
    
    if not text1 or not text2:
        print("âŒ ä¸¤ä¸ªæ–‡æœ¬éƒ½ä¸èƒ½ä¸ºç©º")
        return
    
    models = embedding_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: embedding-3): ").strip() or "embedding-3"
    
    print("ğŸ” è®¡ç®—ä¸­...")
    try:
        result = calculate_text_similarity(text1, text2, model)
        
        if result["success"]:
            print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—æˆåŠŸ!")
            print(f"æ–‡æœ¬1: {result['text1']}")
            print(f"æ–‡æœ¬2: {result['text2']}")
            print(f"ç›¸ä¼¼åº¦: {result['similarity']:.4f}")
            print(f"ç›¸ä¼¼åº¦ç­‰çº§: {result['interpretation']['level']}")
            print(f"æè¿°: {result['interpretation']['description']}")
        else:
            print(f"âŒ è®¡ç®—å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ è®¡ç®—å¤±è´¥: {str(e)}")

def handle_similar_text_search():
    """å¤„ç†ç›¸ä¼¼æ–‡æœ¬æœç´¢"""
    print("\nğŸ” ç›¸ä¼¼æ–‡æœ¬æœç´¢")
    query = input("è¯·è¾“å…¥æŸ¥è¯¢æ–‡æœ¬: ").strip()
    if not query:
        print("âŒ æŸ¥è¯¢æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        return
    
    print("è¯·è¾“å…¥å€™é€‰æ–‡æœ¬ (æ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    candidates = []
    while True:
        text = input().strip()
        if not text:
            break
        candidates.append(text)
    
    if not candidates:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•å€™é€‰æ–‡æœ¬")
        return
    
    models = embedding_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: embedding-3): ").strip() or "embedding-3"
    
    top_k = input("è¯·è¾“å…¥è¿”å›ç»“æœæ•°é‡ (é»˜è®¤: 5): ").strip()
    try:
        top_k = int(top_k) if top_k else 5
    except ValueError:
        top_k = 5
    
    print("ğŸ” æœç´¢ä¸­...")
    try:
        result = find_similar_texts(query, candidates, model, top_k)
        
        if result["success"]:
            print(f"âœ… ç›¸ä¼¼æ–‡æœ¬æœç´¢æˆåŠŸ!")
            print(f"æŸ¥è¯¢: {result['query']}")
            print(f"å€™é€‰æ–‡æœ¬æ•°é‡: {result['total_candidates']}")
            print(f"è¿”å›ç»“æœæ•°é‡: {result['top_k']}")
            print("\næœ€ç›¸ä¼¼çš„æ–‡æœ¬:")
            for i, item in enumerate(result['results']):
                print(f"{i+1}. {item['text']} (ç›¸ä¼¼åº¦: {item['similarity']:.4f})")
        else:
            print(f"âŒ æœç´¢å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ æœç´¢å¤±è´¥: {str(e)}")

def handle_model_info():
    """å¤„ç†æ¨¡å‹ä¿¡æ¯æŸ¥çœ‹"""
    print("\nğŸ”§ æ”¯æŒçš„åµŒå…¥æ¨¡å‹")
    try:
        result = get_supported_embedding_models()
        
        if result["success"]:
            print("âœ… å¯ç”¨çš„æ¨¡å‹:")
            for model in result["models"]:
                info = result["model_info"].get(model, "æ— æè¿°")
                print(f"  {model}: {info}")
            print(f"\né»˜è®¤æ¨¡å‹: {result['default_model']}")
        else:
            print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}")

def handle_api_test():
    """å¤„ç†APIæµ‹è¯•"""
    print("\nğŸ”§ APIè¿æ¥æµ‹è¯•")
    test_text = input("è¯·è¾“å…¥æµ‹è¯•æ–‡æœ¬ (å¯é€‰): ").strip() or None
    
    print("ğŸ” æµ‹è¯•ä¸­...")
    try:
        result = test_embedding_api(test_text)
        
        if result["success"]:
            print("âœ… APIæµ‹è¯•ç»“æœ:")
            conn_test = result["connection_test"]
            print(f"  è¿æ¥çŠ¶æ€: {'æ­£å¸¸' if conn_test else 'å¤±è´¥'}")
            print(f"  æ”¯æŒçš„æ¨¡å‹: {', '.join(result['supported_models'])}")
            
            if 'embedding_test' in result:
                embed_test = result['embedding_test']
                if embed_test['success']:
                    print(f"  æµ‹è¯•åµŒå…¥: æˆåŠŸç”Ÿæˆ {embed_test['dimension']} ç»´å‘é‡")
                    print(f"  ç¤ºä¾‹å€¼: {embed_test['sample_values']}")
                else:
                    print(f"  æµ‹è¯•åµŒå…¥å¤±è´¥: {embed_test['error']}")
        else:
            print(f"âŒ APIæµ‹è¯•å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {str(e)}")

def handle_save_embeddings():
    """å¤„ç†ä¿å­˜åµŒå…¥å‘é‡"""
    print("\nğŸ’¾ ä¿å­˜åµŒå…¥å‘é‡åˆ°æ–‡ä»¶")
    print("è¯·è¾“å…¥è¦ä¿å­˜çš„æ–‡æœ¬ (æ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    
    texts = []
    while True:
        text = input().strip()
        if not text:
            break
        texts.append(text)
    
    if not texts:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•æ–‡æœ¬")
        return
    
    filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ").strip()
    if not filename:
        print("âŒ æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        return
    
    models = embedding_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: embedding-3): ").strip() or "embedding-3"
    
    print("ğŸ’¾ ä¿å­˜ä¸­...")
    try:
        result = save_embeddings_to_file(texts, filename, model)
        
        if result["success"]:
            print(f"âœ… ä¿å­˜æˆåŠŸ!")
            print(f"æ–‡ä»¶è·¯å¾„: {result['file_path']}")
            print(f"æ–‡ä»¶å¤§å°: {result['size']} å­—èŠ‚")
            print(f"æ–‡æœ¬æ•°é‡: {result['count']}")
            print(f"å‘é‡ç»´åº¦: {result['dimension']}")
        else:
            print(f"âŒ ä¿å­˜å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")

def handle_load_embeddings():
    """å¤„ç†åŠ è½½åµŒå…¥å‘é‡"""
    print("\nğŸ“‚ ä»æ–‡ä»¶åŠ è½½åµŒå…¥å‘é‡")
    filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ").strip()
    if not filename:
        print("âŒ æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        return
    
    print("ğŸ“‚ åŠ è½½ä¸­...")
    try:
        result = load_embeddings_from_file(filename)
        
        if result["success"]:
            print(f"âœ… åŠ è½½æˆåŠŸ!")
            print(f"æ–‡ä»¶å: {result['filename']}")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"æ–‡æœ¬æ•°é‡: {result['count']}")
            print(f"å‘é‡ç»´åº¦: {result['dimension']}")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªæ–‡æœ¬
            data = result['data']
            print("\nå‰3ä¸ªæ–‡æœ¬:")
            for i, item in enumerate(data[:3]):
                print(f"{i+1}. {item['text'][:50]}...")
        else:
            print(f"âŒ åŠ è½½å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "--mcp":
            print("ğŸ”§ å¯åŠ¨MCPæœåŠ¡å™¨æ¨¡å¼...")
            mcp.run(transport="sse")
        elif sys.argv[1] == "--test":
            print("ğŸ§ª è¿è¡Œæµ‹è¯•...")
            # å¯ä»¥æ·»åŠ æµ‹è¯•ä»£ç 
            handle_api_test()
        else:
            print("âŒ æœªçŸ¥å‚æ•°ï¼Œæ”¯æŒçš„å‚æ•°: --mcp, --test")
    else:
        # é»˜è®¤è¿è¡Œäº¤äº’å¼æ¨¡å¼
        run_interactive_mode()