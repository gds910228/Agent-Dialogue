"""
AI Text Tokenizer System - Main Entry Point

A comprehensive text tokenization system supporting Zhipu tokenizer models.
Provides both MCP server capabilities and direct text tokenization functionality.
"""

import os
import sys
import time
import json
import uuid
import asyncio
from pathlib import Path
from typing import List, Optional, Dict, Any
from mcp.server.fastmcp import FastMCP
from zhipu_tokenizer_client import ZhipuTokenizerClient
from network_diagnostic import NetworkDiagnostic

# Create an MCP server
mcp = FastMCP("AI Text Tokenizer System")

# Create directories for storing files
OUTPUTS_DIR = Path("outputs")
OUTPUTS_DIR.mkdir(exist_ok=True)

# Load configuration
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = "config.json"
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

config = load_config()

# Initialize clients
api_key = config.get("zhipu_api_key") or os.getenv("ZHIPU_API_KEY")
if not api_key:
    print("è­¦å‘Š: æœªæ‰¾åˆ°æ™ºè°±APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY æˆ–åœ¨config.jsonä¸­é…ç½®")

tokenizer_base_url = config.get("api_settings", {}).get("base_url", "https://open.bigmodel.cn")
tokenizer_client = ZhipuTokenizerClient(api_key=api_key or "", base_url=tokenizer_base_url)

# Text Tokenization Entry Point
class TokenizerGenerator:
    """ä¸»è¦çš„æ–‡æœ¬åˆ†è¯å…¥å£ç±»"""
    
    def __init__(self):
        self.tokenizer_client = tokenizer_client
        self.outputs_dir = OUTPUTS_DIR
    
    def tokenize_text(self, 
                     messages: List[Dict[str, str]],
                     model: str = "glm-4-plus") -> Dict[str, Any]:
        """
        ä¸»è¦çš„æ–‡æœ¬åˆ†è¯å…¥å£
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            model: ä½¿ç”¨çš„åˆ†è¯æ¨¡å‹
            
        Returns:
            åˆ†è¯ç»“æœ
        """
        return self.tokenizer_client.tokenize(
            messages=messages,
            model=model
        )
    
    def get_token_count(self, messages: List[Dict[str, str]], 
                       model: str = "glm-4-plus") -> int:
        """è·å–æ–‡æœ¬çš„tokenæ•°é‡"""
        return self.tokenizer_client.count_tokens_for_messages(messages, model)

# åˆ›å»ºå…¨å±€æ–‡æœ¬åˆ†è¯å®ä¾‹
tokenizer_generator = TokenizerGenerator()

@mcp.tool()
def tokenize_text(
    messages: List[Dict[str, str]],
    model: str = "glm-4-plus"
) -> Dict[str, Any]:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
        model: åˆ†è¯æ¨¡å‹åç§°
    
    Returns:
        åŒ…å«åˆ†è¯ç»“æœçš„å­—å…¸
    """
    try:
        if not messages or len(messages) == 0:
            return {
                "success": False,
                "error": "æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
            }
        
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        for msg in messages:
            if not isinstance(msg, dict) or "role" not in msg or "content" not in msg:
                return {
                    "success": False,
                    "error": "æ¶ˆæ¯æ ¼å¼é”™è¯¯ï¼Œæ¯ä¸ªæ¶ˆæ¯å¿…é¡»åŒ…å«roleå’Œcontentå­—æ®µ"
                }
        
        result = tokenizer_generator.tokenize_text(
            messages=messages,
            model=model
        )
        
        return {
            "success": True,
            "model": model,
            "usage": result.get("usage", {}),
            "request_id": result.get("id", ""),
            "created": result.get("created", 0)
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"æ–‡æœ¬åˆ†è¯å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def get_token_count(
    messages: List[Dict[str, str]],
    model: str = "glm-4-plus"
) -> Dict[str, Any]:
    """
    è·å–æ–‡æœ¬çš„tokenæ•°é‡
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
        model: åˆ†è¯æ¨¡å‹åç§°
    
    Returns:
        åŒ…å«tokenæ•°é‡çš„å­—å…¸
    """
    try:
        if not messages or len(messages) == 0:
            return {
                "success": False,
                "error": "æ¶ˆæ¯åˆ—è¡¨ä¸èƒ½ä¸ºç©º"
            }
        
        # éªŒè¯æ¶ˆæ¯æ ¼å¼
        for msg in messages:
            if not isinstance(msg, dict) or "role" not in msg or "content" not in msg:
                return {
                    "success": False,
                    "error": "æ¶ˆæ¯æ ¼å¼é”™è¯¯ï¼Œæ¯ä¸ªæ¶ˆæ¯å¿…é¡»åŒ…å«roleå’Œcontentå­—æ®µ"
                }
        
        token_count = tokenizer_generator.get_token_count(
            messages=messages,
            model=model
        )
        
        result = {
            "success": True,
            "model": model,
            "token_count": token_count
        }
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–tokenæ•°é‡å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def get_supported_tokenizer_models() -> Dict[str, Any]:
    """
    è·å–æ”¯æŒçš„åˆ†è¯æ¨¡å‹åˆ—è¡¨
    
    Returns:
        åŒ…å«æ”¯æŒæ¨¡å‹çš„ç»“æœå­—å…¸
    """
    try:
        models = tokenizer_client.get_available_models()
        
        return {
            "success": True,
            "models": models,
            "default_model": "glm-4-plus",
            "model_info": {
                "glm-4-plus": "æ™ºè°±AI GLM-4-Plusæ¨¡å‹çš„åˆ†è¯å™¨",
                "glm-4": "æ™ºè°±AI GLM-4æ¨¡å‹çš„åˆ†è¯å™¨",
                "glm-3-turbo": "æ™ºè°±AI GLM-3-Turboæ¨¡å‹çš„åˆ†è¯å™¨"
            }
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"è·å–æ”¯æŒçš„æ¨¡å‹å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def test_tokenizer_api(test_messages: Optional[List[Dict[str, str]]] = None) -> Dict[str, Any]:
    """
    æµ‹è¯•æ–‡æœ¬åˆ†è¯APIè¿æ¥å’ŒåŠŸèƒ½
    
    Args:
        test_messages: å¯é€‰çš„æµ‹è¯•æ¶ˆæ¯åˆ—è¡¨
    
    Returns:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    try:
        # æµ‹è¯•APIè¿æ¥
        connection_test = tokenizer_client.test_connection()
        
        result = {
            "success": True,
            "connection_test": connection_test,
            "supported_models": tokenizer_client.get_available_models()
        }
        
        # å¦‚æœæä¾›äº†æµ‹è¯•æ•°æ®ï¼Œè¿›è¡Œåˆ†è¯æµ‹è¯•
        if test_messages:
            try:
                tokenize_result = tokenizer_client.tokenize(test_messages, "glm-4-plus")
                tokenize_test_result = {
                    "success": True,
                    "message_count": len(test_messages),
                    "token_count": tokenize_result.get("usage", {}).get("prompt_tokens", 0)
                }
                result["tokenize_test"] = tokenize_test_result
            except Exception as e:
                result["tokenize_test"] = {
                    "success": False,
                    "error": str(e)
                }
        
        return result
        
    except Exception as e:
        return {
            "success": False,
            "error": f"APIæµ‹è¯•å¤±è´¥: {str(e)}"
        }

@mcp.tool()
def save_tokenize_results_to_file(
    messages: List[Dict[str, str]],
    filename: str,
    model: str = "glm-4-plus"
) -> Dict[str, Any]:
    """
    å°†åˆ†è¯ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    
    Args:
        messages: æ¶ˆæ¯åˆ—è¡¨
        filename: ä¿å­˜çš„æ–‡ä»¶å
        model: åˆ†è¯æ¨¡å‹åç§°
    
    Returns:
        ä¿å­˜ç»“æœå­—å…¸
    """
    try:
        if not messages or not filename:
            return {
                "success": False,
                "error": "æ¶ˆæ¯åˆ—è¡¨å’Œæ–‡ä»¶åéƒ½æ˜¯å¿…éœ€çš„"
            }
        
        # è·å–åˆ†è¯ç»“æœ
        tokenize_result = tokenizer_generator.tokenize_text(messages, model)
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            "model": model,
            "timestamp": time.time(),
            "messages": messages,
            "usage": tokenize_result.get("usage", {}),
            "request_id": tokenize_result.get("id", ""),
            "created": tokenize_result.get("created", 0)
        }
        
        # åˆ›å»ºå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())[:8]
        name, ext = os.path.splitext(filename)
        if not ext:
            ext = ".json"
        unique_filename = f"{name}_{file_id}{ext}"
        file_path = OUTPUTS_DIR / unique_filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "file_path": str(file_path),
            "filename": unique_filename,
            "size": file_path.stat().st_size,
            "message_count": len(messages),
            "token_count": tokenize_result.get("usage", {}).get("prompt_tokens", 0),
            "model": model
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"ä¿å­˜åˆ†è¯ç»“æœå¤±è´¥: {str(e)}"
        }

@mcp.tool()
def load_tokenize_results_from_file(filename: str) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶åŠ è½½åˆ†è¯ç»“æœ
    
    Args:
        filename: æ–‡ä»¶å
    
    Returns:
        åŠ è½½ç»“æœå­—å…¸
    """
    try:
        if not filename:
            return {
                "success": False,
                "error": "æ–‡ä»¶åä¸èƒ½ä¸ºç©º"
            }
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        file_path = Path(filename)
        if not file_path.exists():
            # å°è¯•åœ¨è¾“å‡ºç›®å½•ä¸­æŸ¥æ‰¾
            output_path = OUTPUTS_DIR / file_path.name
            if output_path.exists():
                file_path = output_path
            else:
                return {
                    "success": False,
                    "error": f"æ–‡ä»¶ä¸å­˜åœ¨: {filename}"
                }
        
        # åŠ è½½æ–‡ä»¶
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return {
            "success": True,
            "filename": file_path.name,
            "model": data.get("model"),
            "message_count": len(data.get("messages", [])),
            "timestamp": data.get("timestamp"),
            "usage": data.get("usage", {}),
            "messages": data.get("messages", [])
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"åŠ è½½åˆ†è¯ç»“æœå¤±è´¥: {str(e)}"
        }

def run_interactive_mode():
    """è¿è¡Œäº¤äº’å¼æ–‡æœ¬åˆ†è¯æ¨¡å¼"""
    print("=" * 60)
    print("ğŸ”¤ AIæ–‡æœ¬åˆ†è¯ç³»ç»Ÿ - äº¤äº’æ¨¡å¼")
    print("=" * 60)
    print("æ”¯æŒçš„åŠŸèƒ½:")
    print("1. æ–‡æœ¬åˆ†è¯")
    print("2. è·å–Tokenæ•°é‡")
    print("3. æŸ¥çœ‹æ”¯æŒçš„æ¨¡å‹")
    print("4. æµ‹è¯•APIè¿æ¥")
    print("5. ä¿å­˜åˆ†è¯ç»“æœåˆ°æ–‡ä»¶")
    print("6. ä»æ–‡ä»¶åŠ è½½åˆ†è¯ç»“æœ")
    print("7. å¯åŠ¨MCPæœåŠ¡å™¨")
    print("0. é€€å‡º")
    print("=" * 60)
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹©åŠŸèƒ½ (0-7): ").strip()
            
            if choice == "0":
                print("ğŸ‘‹ å†è§!")
                break
            elif choice == "1":
                handle_text_tokenize()
            elif choice == "2":
                handle_token_count()
            elif choice == "3":
                handle_model_info()
            elif choice == "4":
                handle_api_test()
            elif choice == "5":
                handle_save_results()
            elif choice == "6":
                handle_load_results()
            elif choice == "7":
                print("ğŸ”§ å¯åŠ¨MCPæœåŠ¡å™¨...")
                mcp.run(transport="sse")
                break
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥0-7")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§!")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

def handle_text_tokenize():
    """å¤„ç†æ–‡æœ¬åˆ†è¯"""
    print("\nğŸ”¤ æ–‡æœ¬åˆ†è¯")
    
    messages = []
    print("è¯·è¾“å…¥æ¶ˆæ¯ (æ ¼å¼: role:contentï¼Œæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    print("ä¾‹å¦‚: user:Hello, how are you?")
    print("      assistant:I'm doing well, thank you for asking!")
    
    while True:
        line = input().strip()
        if not line:
            break
        
        try:
            role, content = line.split(":", 1)
            messages.append({
                "role": role.strip(),
                "content": content.strip()
            })
        except ValueError:
            print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ 'role:content' æ ¼å¼")
    
    if not messages:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•æ¶ˆæ¯")
        return
    
    models = tokenizer_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: glm-4-plus): ").strip() or "glm-4-plus"
    
    print("ğŸ” åˆ†è¯ä¸­...")
    try:
        result = tokenize_text(messages, model)
        
        if result["success"]:
            print(f"âœ… æ–‡æœ¬åˆ†è¯æˆåŠŸ!")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
            print(f"\nä½¿ç”¨æƒ…å†µ:")
            print(f"  Tokenæ•°é‡: {result['usage'].get('prompt_tokens', 0)}")
            print(f"  è¯·æ±‚ID: {result['request_id']}")
            print(f"  åˆ›å»ºæ—¶é—´: {result['created']}")
        else:
            print(f"âŒ åˆ†è¯å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ åˆ†è¯å¤±è´¥: {str(e)}")

def handle_token_count():
    """å¤„ç†è·å–Tokenæ•°é‡"""
    print("\nğŸ”¢ è·å–Tokenæ•°é‡")
    
    messages = []
    print("è¯·è¾“å…¥æ¶ˆæ¯ (æ ¼å¼: role:contentï¼Œæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    print("ä¾‹å¦‚: user:Hello, how are you?")
    print("      assistant:I'm doing well, thank you for asking!")
    
    while True:
        line = input().strip()
        if not line:
            break
        
        try:
            role, content = line.split(":", 1)
            messages.append({
                "role": role.strip(),
                "content": content.strip()
            })
        except ValueError:
            print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ 'role:content' æ ¼å¼")
    
    if not messages:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•æ¶ˆæ¯")
        return
    
    models = tokenizer_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: glm-4-plus): ").strip() or "glm-4-plus"
    
    print("ğŸ” è®¡ç®—ä¸­...")
    try:
        result = get_token_count(messages, model)
        
        if result["success"]:
            print(f"âœ… Tokenè®¡ç®—æˆåŠŸ!")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"æ¶ˆæ¯æ•°é‡: {len(messages)}")
            print(f"Tokenæ•°é‡: {result['token_count']}")
        else:
            print(f"âŒ è®¡ç®—å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ è®¡ç®—å¤±è´¥: {str(e)}")

def handle_model_info():
    """å¤„ç†æ¨¡å‹ä¿¡æ¯æŸ¥çœ‹"""
    print("\nğŸ”§ æ”¯æŒçš„åˆ†è¯æ¨¡å‹")
    try:
        result = get_supported_tokenizer_models()
        
        if result["success"]:
            print("âœ… å¯ç”¨çš„æ¨¡å‹:")
            for model in result["models"]:
                info = result["model_info"].get(model, "æ— æè¿°")
                print(f"  {model}: {info}")
            print(f"\né»˜è®¤æ¨¡å‹: {result['default_model']}")
        else:
            print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {str(e)}")

def handle_api_test():
    """å¤„ç†APIæµ‹è¯•"""
    print("\nğŸ”§ APIè¿æ¥æµ‹è¯•")
    test_messages = None
    
    use_test_data = input("æ˜¯å¦ä½¿ç”¨æµ‹è¯•æ•°æ®? (y/n, é»˜è®¤: y): ").strip().lower() or "y"
    if use_test_data == "y":
        test_messages = [
            {"role": "user", "content": "Hello, how are you?"}
        ]
    
    print("ğŸ” æµ‹è¯•ä¸­...")
    try:
        result = test_tokenizer_api(test_messages)
        
        if result["success"]:
            print("âœ… APIæµ‹è¯•ç»“æœ:")
            conn_test = result["connection_test"]
            print(f"  è¿æ¥çŠ¶æ€: {'æ­£å¸¸' if conn_test else 'å¤±è´¥'}")
            print(f"  æ”¯æŒçš„æ¨¡å‹: {', '.join(result['supported_models'])}")
            
            if 'tokenize_test' in result:
                tokenize_test = result['tokenize_test']
                if tokenize_test['success']:
                    print(f"  æµ‹è¯•åˆ†è¯: æˆåŠŸå¤„ç† {tokenize_test['message_count']} ä¸ªæ¶ˆæ¯")
                    print(f"  Tokenæ•°é‡: {tokenize_test['token_count']}")
                else:
                    print(f"  æµ‹è¯•åˆ†è¯å¤±è´¥: {tokenize_test['error']}")
        else:
            print(f"âŒ APIæµ‹è¯•å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ APIæµ‹è¯•å¤±è´¥: {str(e)}")

def handle_save_results():
    """å¤„ç†ä¿å­˜åˆ†è¯ç»“æœ"""
    print("\nğŸ’¾ ä¿å­˜åˆ†è¯ç»“æœåˆ°æ–‡ä»¶")
    
    messages = []
    print("è¯·è¾“å…¥æ¶ˆæ¯ (æ ¼å¼: role:contentï¼Œæ¯è¡Œä¸€ä¸ªï¼Œç©ºè¡Œç»“æŸ):")
    print("ä¾‹å¦‚: user:Hello, how are you?")
    print("      assistant:I'm doing well, thank you for asking!")
    
    while True:
        line = input().strip()
        if not line:
            break
        
        try:
            role, content = line.split(":", 1)
            messages.append({
                "role": role.strip(),
                "content": content.strip()
            })
        except ValueError:
            print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·ä½¿ç”¨ 'role:content' æ ¼å¼")
    
    if not messages:
        print("âŒ æ²¡æœ‰è¾“å…¥ä»»ä½•æ¶ˆæ¯")
        return
    
    filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ").strip()
    if not filename:
        print("âŒ æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        return
    
    models = tokenizer_client.get_available_models()
    print(f"\nå¯ç”¨çš„æ¨¡å‹: {', '.join(models)}")
    model = input("è¯·é€‰æ‹©æ¨¡å‹ (é»˜è®¤: glm-4-plus): ").strip() or "glm-4-plus"
    
    print("ğŸ’¾ ä¿å­˜ä¸­...")
    try:
        result = save_tokenize_results_to_file(messages, filename, model)
        
        if result["success"]:
            print(f"âœ… ä¿å­˜æˆåŠŸ!")
            print(f"æ–‡ä»¶è·¯å¾„: {result['file_path']}")
            print(f"æ–‡ä»¶å¤§å°: {result['size']} å­—èŠ‚")
            print(f"æ¶ˆæ¯æ•°é‡: {result['message_count']}")
            print(f"Tokenæ•°é‡: {result['token_count']}")
        else:
            print(f"âŒ ä¿å­˜å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")

def handle_load_results():
    """å¤„ç†åŠ è½½åˆ†è¯ç»“æœ"""
    print("\nğŸ“‚ ä»æ–‡ä»¶åŠ è½½åˆ†è¯ç»“æœ")
    filename = input("è¯·è¾“å…¥æ–‡ä»¶å: ").strip()
    if not filename:
        print("âŒ æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        return
    
    print("ğŸ“‚ åŠ è½½ä¸­...")
    try:
        result = load_tokenize_results_from_file(filename)
        
        if result["success"]:
            print(f"âœ… åŠ è½½æˆåŠŸ!")
            print(f"æ–‡ä»¶å: {result['filename']}")
            print(f"æ¨¡å‹: {result['model']}")
            print(f"æ¶ˆæ¯æ•°é‡: {result['message_count']}")
            print(f"Tokenæ•°é‡: {result['usage'].get('prompt_tokens', 0)}")
            
            # æ˜¾ç¤ºæ¶ˆæ¯å†…å®¹
            print("\næ¶ˆæ¯å†…å®¹:")
            for i, msg in enumerate(result['messages']):
                print(f"{i+1}. {msg['role']}: {msg['content'][:50]}...")
        else:
            print(f"âŒ åŠ è½½å¤±è´¥: {result['error']}")
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        if sys.argv[1] == "--mcp":
            print("ğŸ”§ å¯åŠ¨MCPæœåŠ¡å™¨æ¨¡å¼...")
            mcp.run(transport="sse")
        elif sys.argv[1] == "--test":
            print("ğŸ§ª è¿è¡Œæµ‹è¯•...")
            handle_api_test()
        else:
            print("âŒ æœªçŸ¥å‚æ•°ï¼Œæ”¯æŒçš„å‚æ•°: --mcp, --test")
    else:
        # é»˜è®¤è¿è¡Œäº¤äº’å¼æ¨¡å¼
        run_interactive_mode()