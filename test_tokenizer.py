"""
æ™ºè°±AIæ–‡æœ¬åˆ†è¯å™¨æµ‹è¯•è„šæœ¬
ç”¨äºæµ‹è¯•åˆ†è¯åŠŸèƒ½å’ŒAPIè¿æ¥
"""

import os
import json
import time
from typing import List, Dict, Any

from zhipu_tokenizer_client import ZhipuTokenizerClient

def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_path = "config.json"
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

def test_tokenizer():
    """æµ‹è¯•åˆ†è¯åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª æ™ºè°±AIæ–‡æœ¬åˆ†è¯å™¨æµ‹è¯•")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    config = load_config()
    api_key = config.get("zhipu_api_key") or os.getenv("ZHIPU_API_KEY")
    
    if not api_key:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ ZHIPU_API_KEY æˆ–åœ¨config.jsonä¸­é…ç½®")
        return
    
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    base_url = config.get("api_settings", {}).get("base_url", "https://open.bigmodel.cn")
    client = ZhipuTokenizerClient(api_key=api_key, base_url=base_url)
    
    # æµ‹è¯•APIè¿æ¥
    print("ğŸ”„ æµ‹è¯•APIè¿æ¥...")
    if client.test_connection():
        print("âœ… APIè¿æ¥æˆåŠŸ")
    else:
        print("âŒ APIè¿æ¥å¤±è´¥")
        return
    
    # æµ‹è¯•å•æ¡æ¶ˆæ¯åˆ†è¯
    print("\nğŸ”„ æµ‹è¯•å•æ¡æ¶ˆæ¯åˆ†è¯...")
    test_text = "What opportunities and challenges will the Chinese large model industry face in 2025?"
    
    try:
        start_time = time.time()
        token_count = client.count_tokens(test_text)
        elapsed_time = time.time() - start_time
        
        print(f"âœ… åˆ†è¯æˆåŠŸ")
        print(f"æ–‡æœ¬: {test_text}")
        print(f"Tokenæ•°é‡: {token_count}")
        print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ åˆ†è¯å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•å¤šæ¡æ¶ˆæ¯åˆ†è¯
    print("\nğŸ”„ æµ‹è¯•å¤šæ¡æ¶ˆæ¯åˆ†è¯...")
    test_messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about AI development in China."},
        {"role": "assistant", "content": "China has made significant progress in AI research and applications."}
    ]
    
    try:
        start_time = time.time()
        result = client.tokenize(test_messages)
        elapsed_time = time.time() - start_time
        
        token_count = result.get("usage", {}).get("prompt_tokens", 0)
        
        print(f"âœ… åˆ†è¯æˆåŠŸ")
        print(f"æ¶ˆæ¯æ•°é‡: {len(test_messages)}")
        print(f"Tokenæ•°é‡: {token_count}")
        print(f"è¯·æ±‚ID: {result.get('id', 'N/A')}")
        print(f"åˆ›å»ºæ—¶é—´: {result.get('created', 0)}")
        print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ åˆ†è¯å¤±è´¥: {str(e)}")
    
    # æµ‹è¯•ä¸åŒæ¨¡å‹
    print("\nğŸ”„ æµ‹è¯•ä¸åŒæ¨¡å‹...")
    models = client.get_available_models()
    
    for model in models:
        print(f"\næµ‹è¯•æ¨¡å‹: {model}")
        try:
            start_time = time.time()
            token_count = client.count_tokens(test_text, model=model)
            elapsed_time = time.time() - start_time
            
            print(f"âœ… åˆ†è¯æˆåŠŸ")
            print(f"Tokenæ•°é‡: {token_count}")
            print(f"è€—æ—¶: {elapsed_time:.2f}ç§’")
        except Exception as e:
            print(f"âŒ åˆ†è¯å¤±è´¥: {str(e)}")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    test_tokenizer()